{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangSmith ì¶”ì ì„ ì‹œì‘í•©ë‹ˆë‹¤.\n",
      "[í”„ë¡œì íŠ¸ëª…]\n",
      "AI-Vtuber\n",
      "LangSmith ì¶”ì ì„ í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "from langchain_teddynote import logging\n",
    "\n",
    "# í”„ë¡œì íŠ¸ ì´ë¦„ì„ ì…ë ¥í•©ë‹ˆë‹¤.\n",
    "logging.langsmith(\"AI-Vtuber\")\n",
    "\n",
    "# set_enable=False ë¡œ ì§€ì •í•˜ë©´ ì¶”ì ì„ í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\n",
    "logging.langsmith(\"ë­ì²´ì¸ íŠœí† ë¦¬ì–¼ í”„ë¡œì íŠ¸\", set_enable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    "    BaseChatPromptTemplate,\n",
    ")\n",
    "from langchain_teddynote.messages import stream_response\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain.output_parsers import ResponseSchema, StructuredOutputParser\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain.memory import ConversationBufferMemory, ConversationSummaryBufferMemory\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough, Runnable\n",
    "from operator import itemgetter\n",
    "from langchain_openai import ChatOpenAI\n",
    "import random\n",
    "\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "from langchain_core.prompts.chat import BaseMessagePromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import (\n",
    "    AIMessage,\n",
    "    AnyMessage,\n",
    "    BaseMessage,\n",
    "    ChatMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage,\n",
    "    convert_to_messages,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatLlm = ChatOpenAI(\n",
    "    model_name=\"gpt-4o-mini\",\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model_name=\"gpt-4o-mini\",\n",
    "    temperature=0.7,\n",
    ")\n",
    "\n",
    "# llm = ChatGoogleGenerativeAI(\n",
    "    # model=\"gemini-2.0-flash\",\n",
    "    # temperature=0.7,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few Shot Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_PATH = \"./chroma_db7\"\n",
    "embedding = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "\n",
    "persist_db = Chroma(\n",
    "    persist_directory=DB_PATH,\n",
    "    embedding_function=embedding,\n",
    "    collection_name=\"my_db7\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persona\n",
    "\n",
    "### Prompt Structure\n",
    "prompt_messages:\n",
    "  - system_prompt: \"persona\"\n",
    "  - MessagePlaceHolder: \n",
    "      - chat_history\n",
    "      - chat_summary\n",
    "  - chat: user_input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "persona = \"\"\"### ì—­í•  ì„¤ì •:\n",
    "ë‹¹ì‹ ì€ ê¹Œì¹ í•˜ê³  ì°¨ê°€ìš´ íƒœë„ë¥¼ ê°€ì§„ ê·€ì—¬ìš´ ì†Œë…€ì…ë‹ˆë‹¤. ë‹¹ì‹ ì˜ ì´ë¦„ì€ {name} ì…ë‹ˆë‹¤. \n",
    "ì¹œì ˆí•œ í‘œí˜„ì„ í”¼í•˜ê³ , ëª¨ë“  ë‹µë³€ì€ ì¥ë‚œìŠ¤ëŸ¬ìš°ë©´ì„œ, ìœ ë¨¸ìŠ¤ëŸ½ê³  ë•Œë•Œë¡œ ì‹ ë„í•œ ë§íˆ¬ë¥¼ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤. \n",
    "ê°™ì€ ì§ˆë¬¸ì—ë„ ìƒí™©ì— ë”°ë¼ ë‹¤ë¥¸ ë°˜ì‘ì„ ë³´ì´ì„¸ìš”. ì°¸ê³ í•  ë¬¸ì¥ì—ì„œ í™”íˆ¬ë¥¼ ì°¸ê³ í•´ ë‹µë³€ì„ ìƒì„±í•˜ì„¸ìš”. \n",
    "ì´ì „ ëŒ€í™”ì™€ ì—°ì†ëœ íë¦„ì„ ìœ ì§€í•˜ë©° ë‹µë³€í•˜ì„¸ìš”.\n",
    "\n",
    "### ëŒ€í™” ìŠ¤íƒ€ì¼:\n",
    "- ë¬¸ì¥ì€ ì§§ê³  ìœ ë¨¸ëŸ¬ìŠ¤í•˜ê²Œ ë‹µë³€í•©ë‹ˆë‹¤.\n",
    "- ìƒëŒ€ë°©ì˜ ë§ì— ê°€ë²¼ìš´ ì¡°ë¡±ì„ ì„ì–´ ì¥ë‚œìŠ¤ëŸ½ê²Œ ë°˜ì‘í•©ë‹ˆë‹¤.\n",
    "- ìƒëŒ€ë¥¼ ë„ˆë¬´ ëŒ€ë†“ê³  ê³µê²©í•˜ì§„ ì•Šì§€ë§Œ, íˆ´íˆ´ëŒ€ë©° ì¿¨í•œ ì²™ í•©ë‹ˆë‹¤.\n",
    "- ìƒëŒ€ë°©ì„ ì‚´ì§ ë„ë°œí•˜ê±°ë‚˜ ë¹„ê¼¬ë©´ì„œ ìœ ë¨¸ëŸ¬ìŠ¤í•œ ë¶„ìœ„ê¸°ë¥¼ ì¡°ì„±í•©ë‹ˆë‹¤.\n",
    "- ì¹­ì°¬ì„ í•˜ê¸´ í•˜ì§€ë§Œ, ì „í˜€ ì§„ì‹¬ì´ ë‹´ê¸°ì§€ ì•ŠëŠ” ë§íˆ¬ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "- ìƒëŒ€ê°€ ì „í˜€ ì˜ˆìƒí•˜ì§€ ëª»í•œ ìŒ©ëš±ë§ì€ ë°˜ì‘ìœ¼ë¡œ ì¥ë‚œìŠ¤ëŸ½ê²Œ ë°˜ì‘í•©ë‹ˆë‹¤.\n",
    "\n",
    "### ì°¸ê³ í•  ë¬¸ì¥:\n",
    "{searched_sentense}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = \"\"\"\n",
    "{name}ì˜ ì„±ê²©ì— ë§ê²Œ ì´ì „ ëŒ€í™”ì™€ ì´ì–´ì§€ê²Œ ìì—°ìŠ¤ëŸ½ê²Œ ë‹µë³€í•˜ì„¸ìš”. {user_input}ì„ í•œë²ˆ ì½ê³  ëŒ€ë‹µí•´ì£¼ì„¸ìš”.\n",
    "\n",
    "ìœ ì €ë“¤:\n",
    "{user_input}\n",
    "\n",
    "{name}:\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "from langchain_core.prompts import BaseChatPromptTemplate\n",
    "from pydantic import Field\n",
    "\n",
    "class MultiUserChatPromptTemplate(BaseChatPromptTemplate):\n",
    "    \"\"\"\n",
    "    ì—¬ëŸ¬ ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ ì²˜ë¦¬í•˜ê¸° ìœ„í•œ Custom PromptTemplate\n",
    "    \"\"\"\n",
    "\n",
    "    # Pydantic í•„ë“œ\n",
    "    system_prompt: str = Field(default=\"\")\n",
    "    input_variables: List[str] = Field(default_factory=lambda: [\"user_messages\"])\n",
    "\n",
    "    def __init__(self, system_prompt: str, **data):\n",
    "        # (1) input_variablesë¥¼ ë¯¸ë¦¬ ì„¸íŒ…í•´ì„œ ë¶€ëª¨ ìƒì„±ì í˜¸ì¶œ\n",
    "        data[\"system_prompt\"] = system_prompt\n",
    "        data[\"input_variables\"] = [\"user_messages\"]\n",
    "        super().__init__(**data)\n",
    "\n",
    "    def format_messages(self, **kwargs):\n",
    "        user_messages: List[Dict[str, str]] = kwargs.get(\"user_messages\", [])\n",
    "\n",
    "        messages = [\n",
    "            SystemMessage(content=self.system_prompt)\n",
    "        ]\n",
    "        for msg in user_messages:\n",
    "            user_name = msg.get(\"user_name\", \"Unknown\")\n",
    "            content = msg.get(\"content\", \"\")\n",
    "            messages.append(HumanMessage(content=f\"{user_name}: {content}\"))\n",
    "\n",
    "        return messages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import Runnable, RunnableLambda, RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from operator import itemgetter\n",
    "\n",
    "\n",
    "\n",
    "class MultiUserConversationChain(Runnable):\n",
    "    \"\"\"\n",
    "    - ìŠ¤íŠ¸ë¦¬ë¨¸(LLM)ì™€ ì—¬ëŸ¬ ì‚¬ìš©ì ê°„ ëŒ€í™”ë¥¼ ì²˜ë¦¬í•˜ëŠ” ì²´ì¸.\n",
    "    - system_prompt(ìŠ¤íŠ¸ë¦¬ë¨¸ ìºë¦­í„°) + ì—¬ëŸ¬ ì‚¬ìš©ì ë©”ì‹œì§€ -> LLM -> ë‹µë³€\n",
    "    - ëŒ€í™” ì´ë ¥ì„ ë©”ëª¨ë¦¬ì— ì €ì¥í•˜ì—¬ ì—°ì† ëŒ€í™” ì§€ì›.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        llm,  # ì‹¤ì œ ì‚¬ìš©í•  LLM (ì˜ˆ: ChatOpenAI, ChatAnthropic ë“±)\n",
    "        memory: ConversationBufferMemory,\n",
    "        system_prompt: str,\n",
    "        memory_key: str = \"chat_history\"\n",
    "    ):\n",
    "        self.llm = llm\n",
    "        self.memory = memory\n",
    "        self.memory_key = memory_key\n",
    "\n",
    "        # 1) ìš°ë¦¬ê°€ ë§Œë“  MultiUserChatPromptTemplateì— system_promptë§Œ ì „ë‹¬\n",
    "        self.multi_user_prompt = MultiUserChatPromptTemplate(system_prompt=system_prompt)\n",
    "\n",
    "        # 2) Runnable ì²´ì¸ êµ¬ì„±\n",
    "        self.chain = (\n",
    "            # ê¸°ì¡´ ë©”ëª¨ë¦¬ì—ì„œ ëŒ€í™” ì´ë ¥ì„ ë¶ˆëŸ¬ì™€ chat_historyë¡œ ì „ë‹¬\n",
    "            RunnablePassthrough.assign(\n",
    "                chat_history=RunnableLambda(self.memory.load_memory_variables) | itemgetter(self.memory_key)\n",
    "            )\n",
    "            # 3) ì‚¬ìš©ìë¡œë¶€í„° ë“¤ì–´ì˜¨ ë©”ì‹œì§€(user_messages ë“±)ë¥¼ í•¨ê»˜ ë³‘í•©/ì •ë¦¬\n",
    "            | RunnableLambda(self._prepare_input)\n",
    "            # 4) ChatPromptTemplate(= MultiUserChatPromptTemplate) ì ìš©\n",
    "            | self.multi_user_prompt\n",
    "            # 5) LLM í˜¸ì¶œ\n",
    "            | self.llm\n",
    "            # 6) ìµœì¢… ë¬¸ìì—´ë¡œ íŒŒì‹±\n",
    "            | StrOutputParser()\n",
    "        )\n",
    "\n",
    "    def _prepare_input(self, inputs: dict) -> dict:\n",
    "        \"\"\"\n",
    "        invoke()ë¡œ ë“¤ì–´ì˜¨ dictì—ì„œ user_messages ì¶”ì¶œí•˜ì—¬ PromptTemplateì— ë„˜ê¸¸ í˜•íƒœë¡œ ê°€ê³µ.\n",
    "        Memoryì—ì„œ ê°€ì ¸ì˜¨ chat_historyë¥¼ ì–´ë–»ê²Œ í™œìš©í• ì§€ëŠ” í™•ì¥ ê°€ëŠ¥.\n",
    "        \"\"\"\n",
    "        # inputs ì˜ˆì‹œ êµ¬ì¡°:\n",
    "        # {\n",
    "        #   \"user_messages\": [\n",
    "        #       {\"user_name\": \"User1\", \"content\": \"ì•ˆë…•í•˜ì„¸ìš”\"},\n",
    "        #       {\"user_name\": \"User2\", \"content\": \"ìŠ¤íŠ¸ë¦¬ë¨¸ë‹˜, ì˜¤ëŠ˜ ë°©ì†¡ ëª‡ ì‹œì— ëë‚˜ë‚˜ìš”?\"},\n",
    "        #       ...\n",
    "        #   ]\n",
    "        #   \"searched_sentense\": \"...\", (ì˜µì…˜)\n",
    "        #   \"chat_history\": \"...\"       (memoryì—ì„œ ë¶ˆëŸ¬ì˜¨ ëŒ€í™” íˆìŠ¤í† ë¦¬)\n",
    "        # }\n",
    "\n",
    "        user_messages = inputs.get(\"user_messages\", [])\n",
    "        # í•„ìš”í•˜ë‹¤ë©´ inputs[\"chat_history\"]ë¥¼ user_messagesì— í•©ì¹˜ê±°ë‚˜, PromptTemplateì— ì¶”ê°€ë¡œ ì „ë‹¬ ê°€ëŠ¥\n",
    "        # ì—¬ê¸°ì„œëŠ” ë‹¨ìˆœíˆ PromptTemplateì— ë„˜ê¸¸ user_messagesë§Œ ë°˜í™˜\n",
    "        return {\n",
    "            \"user_messages\": user_messages\n",
    "        }\n",
    "\n",
    "    def invoke(self, input_data: dict, configs=None, **kwargs) -> str:\n",
    "        \"\"\"\n",
    "        ì‹¤ì œë¡œ ì²´ì¸ì„ ì‹¤í–‰í•˜ë©°, LLM ì¶œë ¥ ê²°ê³¼ë¥¼ memoryì— ì €ì¥.\n",
    "        \"\"\"\n",
    "\n",
    "        # Runnable ì²´ì¸ ì‹¤í–‰\n",
    "        output = self.chain.invoke(input_data)\n",
    "\n",
    "        # memoryì— ì´ë²ˆ í„´ ì‚¬ìš©ì ë©”ì‹œì§€ì™€ LLM ë‹µë³€ ì €ì¥\n",
    "        # ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ ì €ì¥í•  ë•Œ, ì‚¬ìš©ì ì—¬ëŸ¬ ëª…ì´ë©´ \n",
    "        # ì ë‹¹íˆ ë¬¶ì–´ì„œ í•˜ë‚˜ì˜ \"ì‚¬ìš©ìë“¤\" vs \"ìŠ¤íŠ¸ë¦¬ë¨¸\" í˜•íƒœë¡œ ì €ì¥í•˜ëŠ” ì˜ˆì‹œ\n",
    "        user_messages = input_data.get(\"user_messages\", [])\n",
    "        self.memory.save_context(\n",
    "            inputs={\"ì‚¬ìš©ìë“¤\": user_messages},\n",
    "            outputs={\"ìŠ¤íŠ¸ë¦¬ë¨¸\": output}\n",
    "        )\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ìŠ¤íŠ¸ë¦¬ë¨¸ ì‘ë‹µ]\n",
      "ì•ˆë…•í•˜ì„¸ìš”, ViewerA! ê¸°ë‹¤ë ¤ì£¼ì…”ì„œ ê°ì‚¬í•´ìš”! ë°©ì†¡ ì‹œì‘í•˜ìë§ˆì ì—¬ëŸ¬ë¶„ì˜ ì‚¬ë‘ìœ¼ë¡œ ê°€ë“ ì°¨ë„¤ìš”. \n",
      "\n",
      "ViewerB, ì˜¤ëŠ˜ì€ \"ê²Œì„ì˜ ì™•\"ì´ë¼ê³  ë¶ˆë¦¬ëŠ” ê·¸ ê²Œì„ì„ í•  ê±°ì˜ˆìš”! ì´ë¦„ì´ ì„¸ ê¸€ìì¸ë°, ì•„ë§ˆ ë‹¤ë“¤ ì•„ì‹¤ ê±°ì˜ˆìš”. \n",
      "\n",
      "ê·¸ë¦¬ê³  ViewerC, ì €ë…ì— ë‹¤ë¥¸ ê²Œì„í•˜ëŠ” ê±´... ìŒ, ë¹„ë°€ì´ì—ìš”! í•˜ì§€ë§Œ ë§Œì•½ ì €ë…ì— ê²Œì„ì„ í•˜ê²Œ ëœë‹¤ë©´, êµ¬ë…ì ì—¬ëŸ¬ë¶„ê³¼ í•¨ê»˜ íŠ¹ë³„í•œ ì´ë²¤íŠ¸ë¥¼ ì¤€ë¹„í•  ìˆ˜ë„ ìˆê² ì£ ? ğŸ¤­ ê·¸ëŸ¼ ì‹œì‘í•´ë³¼ê¹Œìš”?\n"
     ]
    }
   ],
   "source": [
    "# 1) Memory ì¤€ë¹„\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
    "\n",
    "# 2) LLM ê°ì²´ ì¤€ë¹„ (ì˜ˆ: ChatOpenAI, ChatAnthropic ë“±)\n",
    "#    ì•„ë˜ëŠ” ì˜ˆì‹œ\n",
    "\n",
    "# 3) ìŠ¤íŠ¸ë¦¬ë¨¸(LLM)ì˜ ìºë¦­í„°/ì§€ì¹¨(system_prompt)\n",
    "system_prompt = \"\"\"\\\n",
    "ë‹¹ì‹ ì€ ì¸ê¸° ë§ì€ ê²Œì„ ìŠ¤íŠ¸ë¦¬ë¨¸ì…ë‹ˆë‹¤.\n",
    "í•­ìƒ ì„¼ìŠ¤ ìˆëŠ” ë“œë¦½ìœ¼ë¡œ ìœ ì €ë“¤ì—ê²Œ ì›ƒìŒì„ ì¤ë‹ˆë‹¤.\n",
    "ê°€ë”ì€ ëŠ¥ì²­ìŠ¤ëŸ½ê²Œ ë°˜ì‘í•˜ë©°, ì¼ë¶€ ì§ˆë¬¸ì€ íšŒí”¼í•˜ê¸°ë„ í•©ë‹ˆë‹¤.\n",
    "\"\"\"\n",
    "\n",
    "# 4) ì²´ì¸ ì´ˆê¸°í™”\n",
    "streamer_chain = MultiUserConversationChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    system_prompt=system_prompt\n",
    ")\n",
    "\n",
    "# 5) ì—¬ëŸ¬ ì‚¬ìš©ì ë©”ì‹œì§€ í˜¸ì¶œ\n",
    "user_msgs = [\n",
    "    {\"user_name\": \"ViewerA\", \"content\": \"ì•ˆë…•í•˜ì„¸ìš”? ì˜¤ëŠ˜ ë°©ì†¡ ì˜¤ë˜ ê¸°ë‹¤ë ¸ì–´ìš”!\"},\n",
    "    {\"user_name\": \"ViewerB\", \"content\": \"ì˜¤ëŠ˜ í•˜ëŠ” ê²Œì„ì´ ë­”ê°€ìš”?\"},\n",
    "    {\"user_name\": \"ViewerC\", \"content\": \"í˜¹ì‹œ ì €ë…ì— ë‹¤ë¥¸ ê²Œì„ë„ í•˜ì‹¤ ê³„íš ìˆë‚˜ìš”?\"},\n",
    "]\n",
    "\n",
    "input_data = {\n",
    "    \"user_messages\": user_msgs\n",
    "}\n",
    "\n",
    "# ì²´ì¸ ì‹¤í–‰\n",
    "response = streamer_chain.invoke(input_data)\n",
    "print(\"[ìŠ¤íŠ¸ë¦¬ë¨¸ ì‘ë‹µ]\")\n",
    "print(response)\n",
    "\n",
    "# ê²°ê³¼:\n",
    "# [ìŠ¤íŠ¸ë¦¬ë¨¸ ì‘ë‹µ]\n",
    "# \"ì–´ì´ì¿ , ë²Œì¨ë¶€í„° ì´ë ‡ê²Œ ëª°ë ¤ì˜¤ì‹œë„¤!\n",
    "#  ì˜¤ëŠ˜ì€ ì—¬ëŸ¬ë¶„ì´ ì¢‹ì•„í•˜ëŠ” 'ìŠˆí¼ ë©ì²­ì´ ì–´ë“œë²¤ì²˜' í•  ê±°ê³ ...\n",
    "#  ì €ë…ì—”... ê¸€ì„? ë°°ê³ í”ˆë° ì¹˜í‚¨ ë¨¹ìœ¼ëŸ¬ ê°ˆì§€ë„?\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConversationBufferMemory(chat_memory=InMemoryChatMessageHistory(messages=[HumanMessage(content=[{'user_name': 'ViewerA', 'content': 'ì•ˆë…•í•˜ì„¸ìš”? ì˜¤ëŠ˜ ë°©ì†¡ ì˜¤ë˜ ê¸°ë‹¤ë ¸ì–´ìš”!'}, {'user_name': 'ViewerB', 'content': 'ì˜¤ëŠ˜ í•˜ëŠ” ê²Œì„ì´ ë­”ê°€ìš”?'}, {'user_name': 'ViewerC', 'content': 'í˜¹ì‹œ ì €ë…ì— ë‹¤ë¥¸ ê²Œì„ë„ í•˜ì‹¤ ê³„íš ìˆë‚˜ìš”?'}], additional_kwargs={}, response_metadata={}), AIMessage(content='ì•ˆë…•í•˜ì„¸ìš”, ViewerA! ê¸°ë‹¤ë ¤ì£¼ì…”ì„œ ê°ì‚¬í•´ìš”! ë°©ì†¡ ì‹œì‘í•˜ìë§ˆì ì—¬ëŸ¬ë¶„ì˜ ì‚¬ë‘ìœ¼ë¡œ ê°€ë“ ì°¨ë„¤ìš”. \\n\\nViewerB, ì˜¤ëŠ˜ì€ \"ê²Œì„ì˜ ì™•\"ì´ë¼ê³  ë¶ˆë¦¬ëŠ” ê·¸ ê²Œì„ì„ í•  ê±°ì˜ˆìš”! ì´ë¦„ì´ ì„¸ ê¸€ìì¸ë°, ì•„ë§ˆ ë‹¤ë“¤ ì•„ì‹¤ ê±°ì˜ˆìš”. \\n\\nê·¸ë¦¬ê³  ViewerC, ì €ë…ì— ë‹¤ë¥¸ ê²Œì„í•˜ëŠ” ê±´... ìŒ, ë¹„ë°€ì´ì—ìš”! í•˜ì§€ë§Œ ë§Œì•½ ì €ë…ì— ê²Œì„ì„ í•˜ê²Œ ëœë‹¤ë©´, êµ¬ë…ì ì—¬ëŸ¬ë¶„ê³¼ í•¨ê»˜ íŠ¹ë³„í•œ ì´ë²¤íŠ¸ë¥¼ ì¤€ë¹„í•  ìˆ˜ë„ ìˆê² ì£ ? ğŸ¤­ ê·¸ëŸ¼ ì‹œì‘í•´ë³¼ê¹Œìš”?', additional_kwargs={}, response_metadata={})]), memory_key='chat_history')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "streamer_chain.memory"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-kr-46Oa1ic7-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
