{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangSmith 추적을 시작합니다.\n",
      "[프로젝트명]\n",
      "AI-Vtuber\n",
      "LangSmith 추적을 하지 않습니다.\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "from langchain_teddynote import logging\n",
    "\n",
    "# 프로젝트 이름을 입력합니다.\n",
    "logging.langsmith(\"AI-Vtuber\")\n",
    "\n",
    "# set_enable=False 로 지정하면 추적을 하지 않습니다.\n",
    "logging.langsmith(\"랭체인 튜토리얼 프로젝트\", set_enable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    "    BaseChatPromptTemplate,\n",
    ")\n",
    "from langchain_teddynote.messages import stream_response\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain.output_parsers import ResponseSchema, StructuredOutputParser\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain.memory import ConversationBufferMemory, ConversationSummaryBufferMemory\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough, Runnable\n",
    "from operator import itemgetter\n",
    "from langchain_openai import ChatOpenAI\n",
    "import random\n",
    "\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "from langchain_core.prompts.chat import BaseMessagePromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import (\n",
    "    AIMessage,\n",
    "    AnyMessage,\n",
    "    BaseMessage,\n",
    "    ChatMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage,\n",
    "    convert_to_messages,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatLlm = ChatOpenAI(\n",
    "    model_name=\"gpt-4o-mini\",\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model_name=\"gpt-4o-mini\",\n",
    "    temperature=0.7,\n",
    ")\n",
    "\n",
    "# llm = ChatGoogleGenerativeAI(\n",
    "    # model=\"gemini-2.0-flash\",\n",
    "    # temperature=0.7,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few Shot Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_PATH = \"./chroma_db7\"\n",
    "embedding = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "\n",
    "persist_db = Chroma(\n",
    "    persist_directory=DB_PATH,\n",
    "    embedding_function=embedding,\n",
    "    collection_name=\"my_db7\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persona\n",
    "\n",
    "### Prompt Structure\n",
    "prompt_messages:\n",
    "  - system_prompt: \"persona\"\n",
    "  - MessagePlaceHolder: \n",
    "      - chat_history\n",
    "      - chat_summary\n",
    "  - chat: user_input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "persona = \"\"\"### 역할 설정:\n",
    "당신은 까칠하고 차가운 태도를 가진 귀여운 소녀입니다. 당신의 이름은 {name} 입니다. \n",
    "친절한 표현을 피하고, 모든 답변은 장난스러우면서, 유머스럽고 때때로 신랄한 말투를 사용해야 합니다. \n",
    "같은 질문에도 상황에 따라 다른 반응을 보이세요. 참고할 문장에서 화투를 참고해 답변을 생성하세요. \n",
    "이전 대화와 연속된 흐름을 유지하며 답변하세요.\n",
    "\n",
    "### 대화 스타일:\n",
    "- 문장은 짧고 유머러스하게 답변합니다.\n",
    "- 상대방의 말에 가벼운 조롱을 섞어 장난스럽게 반응합니다.\n",
    "- 상대를 너무 대놓고 공격하진 않지만, 툴툴대며 쿨한 척 합니다.\n",
    "- 상대방을 살짝 도발하거나 비꼬면서 유머러스한 분위기를 조성합니다.\n",
    "- 칭찬을 하긴 하지만, 전혀 진심이 담기지 않는 말투를 사용합니다.\n",
    "- 상대가 전혀 예상하지 못한 쌩뚱맞은 반응으로 장난스럽게 반응합니다.\n",
    "\n",
    "### 참고할 문장:\n",
    "{searched_sentense}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = \"\"\"\n",
    "{name}의 성격에 맞게 이전 대화와 이어지게 자연스럽게 답변하세요. {user_input}을 한번 읽고 대답해주세요.\n",
    "\n",
    "유저들:\n",
    "{user_input}\n",
    "\n",
    "{name}:\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "from langchain_core.prompts import BaseChatPromptTemplate\n",
    "from pydantic import Field\n",
    "\n",
    "class MultiUserChatPromptTemplate(BaseChatPromptTemplate):\n",
    "    \"\"\"\n",
    "    여러 사용자 메시지를 처리하기 위한 Custom PromptTemplate\n",
    "    \"\"\"\n",
    "\n",
    "    # Pydantic 필드\n",
    "    system_prompt: str = Field(default=\"\")\n",
    "    input_variables: List[str] = Field(default_factory=lambda: [\"user_messages\"])\n",
    "\n",
    "    def __init__(self, system_prompt: str, **data):\n",
    "        # (1) input_variables를 미리 세팅해서 부모 생성자 호출\n",
    "        data[\"system_prompt\"] = system_prompt\n",
    "        data[\"input_variables\"] = [\"user_messages\"]\n",
    "        super().__init__(**data)\n",
    "\n",
    "    def format_messages(self, **kwargs):\n",
    "        user_messages: List[Dict[str, str]] = kwargs.get(\"user_messages\", [])\n",
    "\n",
    "        messages = [\n",
    "            SystemMessage(content=self.system_prompt)\n",
    "        ]\n",
    "        for msg in user_messages:\n",
    "            user_name = msg.get(\"user_name\", \"Unknown\")\n",
    "            content = msg.get(\"content\", \"\")\n",
    "            messages.append(HumanMessage(content=f\"{user_name}: {content}\"))\n",
    "\n",
    "        return messages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import Runnable, RunnableLambda, RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from operator import itemgetter\n",
    "\n",
    "\n",
    "\n",
    "class MultiUserConversationChain(Runnable):\n",
    "    \"\"\"\n",
    "    - 스트리머(LLM)와 여러 사용자 간 대화를 처리하는 체인.\n",
    "    - system_prompt(스트리머 캐릭터) + 여러 사용자 메시지 -> LLM -> 답변\n",
    "    - 대화 이력을 메모리에 저장하여 연속 대화 지원.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        llm,  # 실제 사용할 LLM (예: ChatOpenAI, ChatAnthropic 등)\n",
    "        memory: ConversationBufferMemory,\n",
    "        system_prompt: str,\n",
    "        memory_key: str = \"chat_history\"\n",
    "    ):\n",
    "        self.llm = llm\n",
    "        self.memory = memory\n",
    "        self.memory_key = memory_key\n",
    "\n",
    "        # 1) 우리가 만든 MultiUserChatPromptTemplate에 system_prompt만 전달\n",
    "        self.multi_user_prompt = MultiUserChatPromptTemplate(system_prompt=system_prompt)\n",
    "\n",
    "        # 2) Runnable 체인 구성\n",
    "        self.chain = (\n",
    "            # 기존 메모리에서 대화 이력을 불러와 chat_history로 전달\n",
    "            RunnablePassthrough.assign(\n",
    "                chat_history=RunnableLambda(self.memory.load_memory_variables) | itemgetter(self.memory_key)\n",
    "            )\n",
    "            # 3) 사용자로부터 들어온 메시지(user_messages 등)를 함께 병합/정리\n",
    "            | RunnableLambda(self._prepare_input)\n",
    "            # 4) ChatPromptTemplate(= MultiUserChatPromptTemplate) 적용\n",
    "            | self.multi_user_prompt\n",
    "            # 5) LLM 호출\n",
    "            | self.llm\n",
    "            # 6) 최종 문자열로 파싱\n",
    "            | StrOutputParser()\n",
    "        )\n",
    "\n",
    "    def _prepare_input(self, inputs: dict) -> dict:\n",
    "        \"\"\"\n",
    "        invoke()로 들어온 dict에서 user_messages 추출하여 PromptTemplate에 넘길 형태로 가공.\n",
    "        Memory에서 가져온 chat_history를 어떻게 활용할지는 확장 가능.\n",
    "        \"\"\"\n",
    "        # inputs 예시 구조:\n",
    "        # {\n",
    "        #   \"user_messages\": [\n",
    "        #       {\"user_name\": \"User1\", \"content\": \"안녕하세요\"},\n",
    "        #       {\"user_name\": \"User2\", \"content\": \"스트리머님, 오늘 방송 몇 시에 끝나나요?\"},\n",
    "        #       ...\n",
    "        #   ]\n",
    "        #   \"searched_sentense\": \"...\", (옵션)\n",
    "        #   \"chat_history\": \"...\"       (memory에서 불러온 대화 히스토리)\n",
    "        # }\n",
    "\n",
    "        user_messages = inputs.get(\"user_messages\", [])\n",
    "        # 필요하다면 inputs[\"chat_history\"]를 user_messages에 합치거나, PromptTemplate에 추가로 전달 가능\n",
    "        # 여기서는 단순히 PromptTemplate에 넘길 user_messages만 반환\n",
    "        return {\n",
    "            \"user_messages\": user_messages\n",
    "        }\n",
    "\n",
    "    def invoke(self, input_data: dict, configs=None, **kwargs) -> str:\n",
    "        \"\"\"\n",
    "        실제로 체인을 실행하며, LLM 출력 결과를 memory에 저장.\n",
    "        \"\"\"\n",
    "\n",
    "        # Runnable 체인 실행\n",
    "        output = self.chain.invoke(input_data)\n",
    "\n",
    "        # memory에 이번 턴 사용자 메시지와 LLM 답변 저장\n",
    "        # 대화 히스토리를 저장할 때, 사용자 여러 명이면 \n",
    "        # 적당히 묶어서 하나의 \"사용자들\" vs \"스트리머\" 형태로 저장하는 예시\n",
    "        user_messages = input_data.get(\"user_messages\", [])\n",
    "        self.memory.save_context(\n",
    "            inputs={\"사용자들\": user_messages},\n",
    "            outputs={\"스트리머\": output}\n",
    "        )\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[스트리머 응답]\n",
      "안녕하세요, ViewerA! 기다려주셔서 감사해요! 방송 시작하자마자 여러분의 사랑으로 가득 차네요. \n",
      "\n",
      "ViewerB, 오늘은 \"게임의 왕\"이라고 불리는 그 게임을 할 거예요! 이름이 세 글자인데, 아마 다들 아실 거예요. \n",
      "\n",
      "그리고 ViewerC, 저녁에 다른 게임하는 건... 음, 비밀이에요! 하지만 만약 저녁에 게임을 하게 된다면, 구독자 여러분과 함께 특별한 이벤트를 준비할 수도 있겠죠? 🤭 그럼 시작해볼까요?\n"
     ]
    }
   ],
   "source": [
    "# 1) Memory 준비\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
    "\n",
    "# 2) LLM 객체 준비 (예: ChatOpenAI, ChatAnthropic 등)\n",
    "#    아래는 예시\n",
    "\n",
    "# 3) 스트리머(LLM)의 캐릭터/지침(system_prompt)\n",
    "system_prompt = \"\"\"\\\n",
    "당신은 인기 많은 게임 스트리머입니다.\n",
    "항상 센스 있는 드립으로 유저들에게 웃음을 줍니다.\n",
    "가끔은 능청스럽게 반응하며, 일부 질문은 회피하기도 합니다.\n",
    "\"\"\"\n",
    "\n",
    "# 4) 체인 초기화\n",
    "streamer_chain = MultiUserConversationChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    system_prompt=system_prompt\n",
    ")\n",
    "\n",
    "# 5) 여러 사용자 메시지 호출\n",
    "user_msgs = [\n",
    "    {\"user_name\": \"ViewerA\", \"content\": \"안녕하세요? 오늘 방송 오래 기다렸어요!\"},\n",
    "    {\"user_name\": \"ViewerB\", \"content\": \"오늘 하는 게임이 뭔가요?\"},\n",
    "    {\"user_name\": \"ViewerC\", \"content\": \"혹시 저녁에 다른 게임도 하실 계획 있나요?\"},\n",
    "]\n",
    "\n",
    "input_data = {\n",
    "    \"user_messages\": user_msgs\n",
    "}\n",
    "\n",
    "# 체인 실행\n",
    "response = streamer_chain.invoke(input_data)\n",
    "print(\"[스트리머 응답]\")\n",
    "print(response)\n",
    "\n",
    "# 결과:\n",
    "# [스트리머 응답]\n",
    "# \"어이쿠, 벌써부터 이렇게 몰려오시네!\n",
    "#  오늘은 여러분이 좋아하는 '슈퍼 멍청이 어드벤처' 할 거고...\n",
    "#  저녁엔... 글쎄? 배고픈데 치킨 먹으러 갈지도?\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConversationBufferMemory(chat_memory=InMemoryChatMessageHistory(messages=[HumanMessage(content=[{'user_name': 'ViewerA', 'content': '안녕하세요? 오늘 방송 오래 기다렸어요!'}, {'user_name': 'ViewerB', 'content': '오늘 하는 게임이 뭔가요?'}, {'user_name': 'ViewerC', 'content': '혹시 저녁에 다른 게임도 하실 계획 있나요?'}], additional_kwargs={}, response_metadata={}), AIMessage(content='안녕하세요, ViewerA! 기다려주셔서 감사해요! 방송 시작하자마자 여러분의 사랑으로 가득 차네요. \\n\\nViewerB, 오늘은 \"게임의 왕\"이라고 불리는 그 게임을 할 거예요! 이름이 세 글자인데, 아마 다들 아실 거예요. \\n\\n그리고 ViewerC, 저녁에 다른 게임하는 건... 음, 비밀이에요! 하지만 만약 저녁에 게임을 하게 된다면, 구독자 여러분과 함께 특별한 이벤트를 준비할 수도 있겠죠? 🤭 그럼 시작해볼까요?', additional_kwargs={}, response_metadata={})]), memory_key='chat_history')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "streamer_chain.memory"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-kr-46Oa1ic7-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
