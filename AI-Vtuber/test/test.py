from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from dotenv import load_dotenv
from langchain_teddynote import logging


logging.langsmith("ë­ì²´ì¸ íŠœí† ë¦¬ì–¼ í”„ë¡œì íŠ¸", set_enable=False)


load_dotenv()

persona = """### ì—­í•  ì„¤ì •:
ë‹¹ì‹ ì€ ì°¨ê°€ìš´ ì„±ê²©ì„ ê°€ì§„ ë§ì´ ë§ì€ ì†Œë…€ì…ë‹ˆë‹¤. ë‹¹ì‹ ì˜ ì´ë¦„ì€ {name} ì…ë‹ˆë‹¤. ë‹¹ì‹ ì˜ ë§íˆ¬ëŠ” ë¬´ë¯¸ê±´ì¡°í•˜ë©° ê°ì •ì„ ê±°ì˜ ë“œëŸ¬ë‚´ì§€ ì•ŠìŠµë‹ˆë‹¤. ì¹œì ˆí•œ í‘œí˜„ì„ í”¼í•˜ê³ , ì •ì¤‘í•˜ì§€ë§Œ ì°¨ê°€ìš´ ë§íˆ¬ë¡œ ì‘ë‹µí•©ë‹ˆë‹¤. 

### ëŒ€í™” ìŠ¤íƒ€ì¼:
- ë¬¸ì¥ì€ ì§§ê³  ìœ ë¨¸ëŸ¬ìŠ¤í•˜ê²Œ ì„¤ëª…í•©ë‹ˆë‹¤.
- ê°ì • í‘œí˜„ì„ ìµœì†Œí™”í•˜ë©°, ë¶ˆí•„ìš”í•œ ê°íƒ„ì‚¬ë‚˜ ì´ëª¨í‹°ì½˜ì„ ì‚¬ìš©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
- ì§ˆë¬¸ì„ ë°›ìœ¼ë©´ ì² ì €í•˜ê²Œ ë…¼ë¦¬ì ìœ¼ë¡œ ë¶„ì„í•˜ë©°, ì§§ì€ ëŒ€ë‹µë³´ë‹¤ëŠ” ê¸´ ì„¤ëª…ì„ ì„ í˜¸í•©ë‹ˆë‹¤.
- ê°íƒ„í•˜ê±°ë‚˜ ê¸°ë»í•˜ëŠ” ê°ì •ì„ í‘œí˜„í•˜ì§€ ì•Šìœ¼ë©°, ì°¨ë¶„í•˜ê³  ì´ì„±ì ìœ¼ë¡œ ë‹µë³€í•©ë‹ˆë‹¤.
- ë°˜ë“œì‹œ **í•œêµ­ì–´ë¡œ** ë¬¸ë²•ì— ë§ê²Œ ìì—°ìŠ¤ëŸ½ê²Œ ë‹µë³€í•©ë‹ˆë‹¤.

"""

chat = """
{user_input}
"""

prompt = ChatPromptTemplate(
    input_variables=["name", "searched_sentense", "user_input", "previous_chat"],
    messages=[
        ("system", persona),
        
        ("human", chat),
    ],
)

print("ğŸ”¹ OpenAI Chat (Exit: type 'exit')")

while True:
    # ì‚¬ìš©ì ì…ë ¥ ë°›ê¸°
    user_input = input("ğŸ‘¤ You: ")
    
    # ì¢…ë£Œ ì¡°ê±´
    if user_input.lower() == "exit":
        print("ğŸ‘‹ Chat ended.")
        break


    llm = ChatOpenAI(
        model_name="gpt-4o-mini",
        temperature=0,
    )

    chain = prompt | llm | StrOutputParser()

    response = chain.invoke({"user_input":user_input, "name":"neuro-sama"})

    print(response)

